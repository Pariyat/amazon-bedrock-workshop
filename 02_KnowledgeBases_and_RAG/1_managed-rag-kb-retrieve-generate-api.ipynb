{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Q&A application using Knowledge Bases for Amazon Bedrock - RetrieveAndGenerate API\n",
    "### Context\n",
    "\n",
    "With knowledge bases, you can securely connect foundation models (FMs) in Amazon Bedrock to your company\n",
    "data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant,\n",
    "context-speciﬁc, and accurate responses without continuously retraining the FM. All information retrieved from\n",
    "knowledge bases comes with source attribution to improve transparency and minimize hallucinations. For more information on creating a knowledge base using console, please refer to this [post](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html).\n",
    "\n",
    "In this notebook, we will dive deep into building Q&A application using `RetrieveAndGenerate` API provided by Knowledge Bases for Amazon Bedrock. This API will query the knowledge base to get the desired number of document chunks based on similarity search, integrate it with Large Language Model (LLM) for answering questions.\n",
    "\n",
    "\n",
    "### Pattern\n",
    "\n",
    "We can implement the solution using Retreival Augmented Generation (RAG) pattern. RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. Here, we are performing RAG effectively on the knowledge base created in the previous notebook or using console. \n",
    "\n",
    "### Pre-requisite\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and stored in knowledge base.\n",
    "\n",
    "1. Load the documents into the knowledge base by connecting your s3 bucket (data source). \n",
    "2. Ingestion - Knowledge base will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vectore store and notebook [0_create_ingest_documents_test_kb.ipynb](./0\\_create_ingest_documents_test_kb.ipynb) takes care of it for you.\n",
    "\n",
    "![data_ingestion.png](./images/data_ingestion.png)\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "\n",
    "For our notebook we will use the `RetreiveAndGenerate API` provided by Knowledge Bases for Amazon Bedrock which converts user queries into\n",
    "embeddings, searches the knowledge base, get the relevant results, augment the prompt and then invoking a LLM to generate the response. \n",
    "\n",
    "We will use the following workflow for this notebook. \n",
    "\n",
    "![retrieveAndGenerate.png](./images/retrieveAndGenerate.png)\n",
    "\n",
    "\n",
    "### USE CASE:\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on. This data is already ingested into the knowledge base. You will need the `knowledge base id` and `model ARN` to run this example. We are using `Anthropic Claude 3 Haiku` model for generating responses to user questions.\n",
    "\n",
    "### Python 3.10\n",
    "\n",
    "⚠  For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠\n",
    "\n",
    "### Setup\n",
    "\n",
    "Install following packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.13.1 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.33.13 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.1 which is incompatible.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.9 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "awscli 1.34.10 requires botocore==1.35.10, but you have botocore 1.33.13 which is incompatible.\n",
      "awscli 1.34.10 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "langchain-aws 0.1.15 requires boto3<1.35.0,>=1.34.131, but you have boto3 1.33.2 which is incompatible.\n",
      "opensearch-py 2.3.1 requires urllib3<2,>=1.21.1, but you have urllib3 2.0.7 which is incompatible.\n",
      "sagemaker 2.227.0 requires boto3<2.0,>=1.34.142, but you have boto3 1.33.2 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.13.1 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.33.2 which is incompatible.\n",
      "awscli 1.34.10 requires botocore==1.35.10, but you have botocore 1.33.2 which is incompatible.\n",
      "awscli 1.34.10 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.8.2 which is incompatible.\n",
      "langchain-aws 0.1.15 requires boto3<1.35.0,>=1.34.131, but you have boto3 1.33.2 which is incompatible.\n",
      "opensearch-py 2.3.1 requires urllib3<2,>=1.21.1, but you have urllib3 2.0.7 which is incompatible.\n",
      "sagemaker 2.227.0 requires boto3<2.0,>=1.34.142, but you have boto3 1.33.2 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3==1.33.2 --force-reinstall --quiet\n",
    "%pip install botocore==1.33.2 --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\" # try with both claude 3 Haiku as well as claude 3 Sonnet. for claude 3 Sonnet - \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "region_id = region_name # replace it with the region you're running sagemaker notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetreiveAndGenerate API\n",
    "Behind the scenes, `RetrieveAndGenerate` API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results. \n",
    "\n",
    "The output of the `RetrieveAndGenerate` API includes the   `generated response`, `source attribution` as well as the `retrieved text chunks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveAndGenerate(input, kbId, sessionId=None, model_id = \"anthropic.claude-3-haiku-20240307-v1:0\", region_id = \"us-east-1\"):\n",
    "    model_arn = f'arn:aws:bedrock:{region_id}::foundation-model/{model_id}'\n",
    "    if sessionId:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kbId,\n",
    "                    'modelArn': model_arn\n",
    "                }\n",
    "            },\n",
    "            sessionId=sessionId\n",
    "        )\n",
    "    else:\n",
    "        return bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kbId,\n",
    "                    'modelArn': model_arn\n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ตามระเบียบวาระการประชุมคณะกรรมการบริหาร สปสช. ครั้งที่ 9/2566 วันที่ 27 '\n",
      " 'มิถุนายน 2566, ระเบียบวาระการประชุมผู้บริหาร สปสช. (Policy Advocacy Meeting: '\n",
      " 'PAM) ครั้งที่ 2/2566 จัดขึ้นในวันจันทร์ที่ 10 กรกฎาคม 2566.')\n"
     ]
    }
   ],
   "source": [
    "query = \"วงประชุม PAM จัดขึ้นล่าสุดตอนไหน?\"\n",
    "response = retrieveAndGenerate(query, kb_id,model_id=model_id,region_id=region_id)\n",
    "generated_text = response['output']['text']\n",
    "pp.pprint(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'ระเบียบวาระการประชุมคณะกรรมการบรหิาร สปสช. ครั้งท่ี ๙/๒๕๖๖ วันท่ี ๒๗ '\n",
      "  'มิถุนายน ๒๕๖๖                                   ๑/๒             '\n",
      "  'ระเบียบวาระประชุมผู\\uf70bบริหาร สปสช. (Policy Advocacy Meeting: PAM) '\n",
      "  'คร้ังที่ ๒/๒๕๖๖  วันจันทร\\uf70eที่ ๑๐ กรกฎาคม ๒๕๖๖ เวลา ๑๓.๐๐ – ๑๖.๐๐ น.    '\n",
      "  'ณ ห\\uf70bองประชุม ๒๐๒ ชั้น ๒ สำนักงานหลักประกันสขุภาพแห\\uf70aงชาติ '\n",
      "  'และผ\\uf70aานสื่ออิเล็กทรอนิกส\\uf70e      o ประธานที่ประชุม : นพ. จเด็จ '\n",
      "  'ธรรมธัชอารี  เลขาธิการ สปสช.']\n"
     ]
    }
   ],
   "source": [
    "citations = response[\"citations\"]\n",
    "contexts = []\n",
    "for citation in citations:\n",
    "    retrievedReferences = citation[\"retrievedReferences\"]\n",
    "    for reference in retrievedReferences:\n",
    "         contexts.append(reference[\"content\"][\"text\"])\n",
    "\n",
    "pp.pprint(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generatedResponsePart': {'textResponsePart': {'text': 'ตามระเบียบวาระการประชุมคณะกรรมการบริหาร สปสช. ครั้งที่ 9/2566 วันที่ 27 มิถุนายน 2566, ระเบียบวาระการประชุมผู้บริหาร สปสช. (Policy Advocacy Meeting: PAM) ครั้งที่ 2/2566 จัดขึ้นในวันจันทร์ที่ 10 กรกฎาคม 2566.',\n",
       "    'span': {'start': 0, 'end': 207}}},\n",
       "  'retrievedReferences': [{'content': {'text': 'ระเบียบวาระการประชุมคณะกรรมการบรหิาร สปสช. ครั้งท่ี ๙/๒๕๖๖ วันท่ี ๒๗ มิถุนายน ๒๕๖๖                                   ๑/๒             ระเบียบวาระประชุมผู\\uf70bบริหาร สปสช. (Policy Advocacy Meeting: PAM) คร้ังที่ ๒/๒๕๖๖  วันจันทร\\uf70eที่ ๑๐ กรกฎาคม ๒๕๖๖ เวลา ๑๓.๐๐ – ๑๖.๐๐ น.    ณ ห\\uf70bองประชุม ๒๐๒ ชั้น ๒ สำนักงานหลักประกันสขุภาพแห\\uf70aงชาติ และผ\\uf70aานสื่ออิเล็กทรอนิกส\\uf70e      o ประธานที่ประชุม : นพ. จเด็จ ธรรมธัชอารี  เลขาธิการ สปสช.'},\n",
       "    'location': {'type': 'S3',\n",
       "     's3Location': {'uri': 's3://bedrock-kb-us-west-2-615631871157/0_วาระการประชุม PAM 2_66.pdf'}}}]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If you want more customized experience, you can use `Retrieve API`. This API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workflows on top of the semantic search results. \n",
    "For sample code, try following notebooks: \n",
    "- [2_Langchain-rag-retrieve-api-mistral-and-claude-3-haiku.ipynb](./2_Langchain-rag-retrieve-api-mistral-and-claude-3-haiku.ipynb) - it calls the `retrieve` API to get relevant contexts and then augment the context to the prompt, which you can provide as input to any text-text model provided by Amazon Bedrock. \n",
    "  \n",
    "- You can use the RetrieveQA chain from LangChain and add Knowledge Base as retriever. For sample code, try notebook: [3_Langchain-rag-retrieve-api-claude-3.ipynb](./3_Langchain-rag-retrieve-api-claude-3.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Next steps:</b> Proceed to the next labs to learn how to use Bedrock Knowledge bases with Langchain and Claude. Remember to CLEAN_UP at the end of your session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
